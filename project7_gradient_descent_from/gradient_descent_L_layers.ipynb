{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent from Scratch for L-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_deep_nn(layers):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    layers : list of units in each layer of the network \n",
    "    \n",
    "    Outpits\n",
    "    initial_vals : dictionary with \"W_1\", \"b_1\",, 'W_2', 'b_2' ..., 'W_L', 'b_L' where \n",
    "                   W_l -- weight matrix of lth layer (shape: layers_dim[l], layers_dim[l-1])\n",
    "                   b_l -- bias of lth layer(shape: layers_dim[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    params = {}\n",
    "    L = len(layers)\n",
    "    for l in range(1, L):  \n",
    "        #initialize to small(to avoid preventing slow learning due to large weights) \n",
    "        #normal random variables with mean 0\n",
    "        params['W_' + str(l)] = np.random.randn(layers[l], layers[l-1])*0.01\n",
    "        #we can initialize the biases to zero\n",
    "        params['b_' + str(l)] = np.zeros((layers[l],1))        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_output(Z, activation_type):\n",
    "    \n",
    "    if activation_type=='relu':\n",
    "        A=np.maximum(0,Z)\n",
    "    elif activation_type=='sigmoid':\n",
    "        A=1/(1+np.exp(-Z))\n",
    "    elif activation_type=='tanh':\n",
    "        a1, a2 = np.exp(Z), np.exp(-Z)\n",
    "        A= (a1-a2)/(a1+a2)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_L_layers(X, params):\n",
    "    \"\"\"\n",
    "    Forward propagation steps of L-layer deep nn.\n",
    "    \n",
    "    Inputs\n",
    "    X : data, numpy array  (shape : input size, number of examples)\n",
    "    params : initialized values of weights and biases from initialize_deep_nn\n",
    "    \n",
    "    Outputs:\n",
    "    A_L : final layer's activation output\n",
    "    caches : list of caches where cache[k]= ( A_str(k), W_str(k+1), b_str(k+1), Z_str(k+1) ) k=0,1...L-1\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for k in range(1, L): \n",
    "        Z_k= np.dot(params[\"W_\"+str(k)], A) + params[\"b_\"+str(k)]\n",
    "        cache=(A, params[\"W_\"+str(k)], params[\"b_\"+str(k)] ,  Z_k)\n",
    "        caches.append(cache)\n",
    "        #one can enter 'sigmoid' or 'tanh', too.\n",
    "        #A is updated to be the activation of kth layer. It has been activation of k-1'th unit so far.\n",
    "        A= activation_output(Z_k,'relu')\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Suppose we are doing binary classification and the last layer is a sigmoid unit.\n",
    "    Z=np.dot(params[\"W_\"+str(L)], A)+params[\"b_\"+str(L)]\n",
    "    cache=( A, params[\"W_\"+str(L)] , params[\"b_\"+str(L)] ,  Z )\n",
    "    caches.append(cache)\n",
    "    A_L= activation_output(Z,'sigmoid')\n",
    "        \n",
    "    return A_L, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_logistic_cost (A_L, Y):\n",
    "    \"\"\"\n",
    "    For this example, assume the cost is cross entropy cost.\n",
    "\n",
    "    Inputs\n",
    "    A_L : probability vector of probabilities that a certain example is of class 1\n",
    "            , shape (shape: 1, number of examples)\n",
    "    Y : vector of ground truth labels of 1's or 0's, (shape : 1, number of examples)\n",
    "   \n",
    "    \n",
    "    Outputs\n",
    "    cost : cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -1/m * np.sum(Y* np.log(AL) + (1-Y)*np.log(1-AL)) \n",
    "    return np.squeeze(cost)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_prop_L_layers(A_L, Y, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    A_L : probability vector of probabilities that a certain example is of class 1 (shape: 1, number of examples)\n",
    "    Y : vector of ground truth labels of 1's or 0's, (shape : 1, number of examples)\n",
    "    caches : list of caches where cache[k]= ( A_str(k), W_str(k), b_str(k)), Z_str(k) ) k=1,2...L\n",
    "    \n",
    "    Outputs:\n",
    "    gradients: dictionary of gradients to be used in updates: \n",
    "                gradients['dW_'+str(k)]\n",
    "                gradients['db_'+str(k)]     k=1,...L\n",
    "    \"\"\"\n",
    "    \n",
    "    gradients = {}     #dictionary to hold the gradients\n",
    "    L = len(caches)   #number of layers in the model\n",
    "    m = A_L.shape[1]  #number of samples\n",
    "    Y = Y.reshape(A_L.shape) # just in case convert Y to the same shape as A_l\n",
    "    \n",
    "    \n",
    "    #last layer assumed to be sigmoid so \n",
    "    dA_L = - (np.divide(Y, A_L) - np.divide(1 - Y, 1 - A_L))\n",
    "    \n",
    "    #A_L_1 stands for A for L-1'th layer\n",
    "    A_L_1, W_L, b_L, Z_L = caches[-1]\n",
    "    #compute dZ of L-1 th layer \n",
    "    g= 1/(1+np.exp(-Z_L))\n",
    "    dZ_L= dA_L * g *(1-g)\n",
    "    #using dZ, W_L, b_L, Z_L compute the gradients\n",
    "    dW_L = 1/m * np.dot(dZ_L, A_L_1.T)\n",
    "    db_L = 1/m * np.sum(dZ_L, axis=1, keepdims=True)\n",
    "    dA_L_1 = np.dot(W_L.T, dZ_L)\n",
    "    \n",
    "    #store them in gradients dictionary\n",
    "    dA=dA_L_1\n",
    "    gradients[\"dW_\" + str(L)]=dW_L\n",
    "    gradients[\"db_\" + str(L)]=db_L\n",
    "    \n",
    "    for k in reversed(range(1,L)):\n",
    "        \n",
    "        #A of (k-1)th layer, W of kth layer, b of kth layer and Z of kth layer \n",
    "        A, W, b, Z = caches[k-1]\n",
    "        \n",
    "        #we assume relu activation functions in all layers. so the derivative of relu activation function\n",
    "        #will be used to compute dZ_k from dA_k\n",
    "        g_prime=np.ones((Z.shape))\n",
    "        g_prime[Z<0]=0\n",
    "        dZ= dA * g_prime\n",
    "        m=dA.shape[1]\n",
    "        dW = 1/m * np.dot(dZ, A.T)\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        #now update dA so that it corresponds to dA of k-1'th layer instead of dA of k'th layer\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        \n",
    "        gradients[\"dW_\" + str(k)]=dW\n",
    "        gradients[\"db_\" + str(k)]=db\n",
    "    \n",
    "    return gradients\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, lr, gradients):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    params: dictionary holding weights and biases params['W_'+str(k)] and params['b_'+str(k)] k=1,2...L\n",
    "    gradients: dictionary holding gradients of weights and biases gradients[\"dW_\" + str(k)]\n",
    "    lr: learning rate, a positive(small) scalar\n",
    "    \n",
    "    Outputs:\n",
    "    params: updated dictionary holding weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in range(len(params)//2):\n",
    "        params[\"W_\" + str(k+1)] -= lr* gradients[\"dW_\"+ str(k+1)]\n",
    "        params[\"b_\" + str(k+1)] -= lr* gradients[\"db_\"+ str(k+1)]\n",
    "        \n",
    "    return params\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
