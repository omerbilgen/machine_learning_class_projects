{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Project 1: $k$-Nearest Neighbors</h2>\n",
    "<p><cite><center>So many points,<br>\n",
    "some near some far,<br>\n",
    "- who are my true neighbors?</center></cite></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>In this project, you will build a $k$-nearest neighbor classifier.</p>\n",
    "\n",
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.\n",
    "\n",
    "<p><strong>Evaluation:</strong> Your code will be autograded for technical\n",
    "correctness and--on some assignments--speed. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. Furthermore, <em>any code not surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags will not be run by the autograder</em>. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\n",
    "\n",
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "\n",
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/icxgflcnpra3ko\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Libraries**: Before we get started we need to install a few libraries. You can do this by executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "#</GRADED>\n",
    "import sys\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from helper_functions import loaddata, visualize_knn_2D, visualize_knn_images, plotfaces, visualize_knn_boundary\n",
    "\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> k-Nearest Neighbors implementation in Python </h3>\n",
    "\n",
    "<p>Our first goal towards a $k$NN classifier is to build a classifier for handwritten digits classification and face recognition. \n",
    "</p>\n",
    "\n",
    "**Data:** We first obtain some data for testing your code. The data resides in the files <code>faces.mat</code> and <code>digits.mat</code> which hold the datasets for the further experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here, <b>xTr</b> are the training vectors with labels <b>yTr</b> and <b>xTe</b> are the testing vectors with labels <b>yTe</b>. \n",
    "As a reminder, to predict the label or class of an image in <b>xTe</b>, we will look for the <i>k</i>-nearest neighbors in <b>xTr</b> and predict a label based on their labels in <b>yTr</b>. For evaluation, we will compare these labels against the true labels provided in <b>yTe</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h4> Visualizing data</h4>\n",
    "\n",
    "Let us take a look at our data. The following script will take the first 10 training images from the face data set and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=loaddata(\"faces.mat\")\n",
    "\n",
    "plt.figure()\n",
    "plotfaces(xTr[:9, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "<h4> Implementation </h4>\n",
    "<p> The following questions will ask you to finish these functions in a pre-defined order. <br></p>\n",
    "\n",
    "<p>(a) Implement the function  <b><code>l2distance</code></b>. You may use your own code(s) from the previous project.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def l2distance(X,Z=None):\n",
    "    # function D=l2distance(X,Z)\n",
    "    #\n",
    "    # Computes the Euclidean distance matrix.\n",
    "    # Syntax:\n",
    "    # D=l2distance(X,Z)\n",
    "    # Input:\n",
    "    # X: nxd data matrix with n vectors (rows) of dimensionality d\n",
    "    # Z: mxd data matrix with m vectors (rows) of dimensionality d\n",
    "    #\n",
    "    # Output:\n",
    "    # Matrix D of size nxm\n",
    "    # D(i,j) is the Euclidean distance of X(i,:) and Z(j,:)\n",
    "    #\n",
    "    # call with only one input:\n",
    "    # l2distance(X)=l2distance(X,X)\n",
    "    #\n",
    "\n",
    "    if Z is None:\n",
    "        Z=X;\n",
    "\n",
    "    n,d1=X.shape\n",
    "    m,d2=Z.shape\n",
    "    assert (d1==d2), \"Dimensions of input vectors must match!\"\n",
    "\n",
    "   \n",
    "    S = np.diag(X @ (X.T)).reshape(n, 1)\n",
    "    R = np.diag(Z @ (Z.T)).reshape(1,m)\n",
    "    D = np.sqrt(-2 * (X @ Z.T) + S + R)\n",
    "\n",
    "    return D\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>(b) Implement the function <b><code>findknn</code></b>, which should find the $k$ nearest neighbors of a set of vectors within a given training data set. The call of \n",
    "<pre>\n",
    " [I,D]=findknn(xTr,xTe,k);\n",
    "</pre> \n",
    "should result in two matrices $I$ and $D$, both of dimensions $k\\times n$, where $n$ is the number of input vectors in <code>xTe</code>. The matrix $I(i,j)$ is the index of the $i^{th}$ nearest neighbor of the vector $xTe(j,:)$. \n",
    "So, for example, if we set <code>i=I(1,3)</code>, then <code>xTr(i,:)</code> is the first nearest neighbor of vector <code>xTe(3,:)</code>. The second matrix $D$ returns the corresponding distances. So $D(i,j)$ is the distance of $xTe(j,:)$ to its $i^{th}$ nearest neighbor.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def findknn(xTr,xTe,k):\n",
    "    \"\"\"\n",
    "    function [indices,dists]=findknn(xTr,xTe,k);\n",
    "    \n",
    "    Finds the k nearest neighbors of xTe in xTr.\n",
    "    \n",
    "    Input:\n",
    "    xTr = nxd input matrix with n row-vectors of dimensionality d\n",
    "    xTe = mxd input matrix with m row-vectors of dimensionality d\n",
    "    k = number of nearest neighbors to be found\n",
    "    \n",
    "    Output:\n",
    "    indices = kxm matrix, where indices(i,j) is the i^th nearest neighbor of xTe(j,:)\n",
    "    dists = Euclidean distances to the respective nearest neighbors\n",
    "    \"\"\"\n",
    "    # Enter your code here\n",
    "    D = l2distance(xTr, xTe)\n",
    "    indices = (D.argsort(axis=0))[:k, :]\n",
    "    D.sort(axis=0)\n",
    "    dists = D[:k, :]\n",
    "    #raise NotImplementedError('Your code goes here!')\n",
    "\n",
    "    return indices, dists\n",
    "    # until here\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The following demo samples random points in 2D. If your findknn  function is correctly implemented, you should be able to click anywhere on the plot to add a test point. The function should then draw direct connections from your test point to the k  nearest neighbors. Verify manually if your code is correct.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_knn_2D(findknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We can visualize the k=3 nearest training neighbors of some of the test points (Click on the image to cycle through different test points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_knn_images(findknn, imageType='faces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(c) The function <b><code>analyze</code></b> should compute various metrics to evaluate a classifier. The call of\n",
    "<pre>\n",
    "  result=analyze(kind,truth,preds);\n",
    "</pre>\n",
    "should output the <b>accuracy</b> or <b>absolute loss</b> in variable <code>result</code>. The type of output required can be specified in the input argument <code>kind</code> as <code>\"abs\"</code> or <code>\"acc\"</code>. The input variables <code>truth</code> and <code>pred</code> should contain vectors of true and predicted labels respectively.\n",
    "For example, the call\n",
    "<pre>\n",
    ">> analyze('acc',[1 2 1 2],[1 2 1 1])\n",
    "</pre>\n",
    "should return an accuracy of 0.75. Here, the true labels are 1,2,1,2 and the predicted labels are 1,2,1,1. So the first three examples are classified correctly, and the last one is wrong --- 75% accuracy.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def analyze(kind,truth,preds):\n",
    "    \"\"\"\n",
    "    function output=analyze(kind,truth,preds)         \n",
    "    Analyses the accuracy of a prediction\n",
    "    Input:\n",
    "    kind='acc' classification error\n",
    "    kind='abs' absolute loss\n",
    "    (other values of 'kind' will follow later)\n",
    "    \"\"\"\n",
    "    \n",
    "    truth = truth.flatten()\n",
    "    preds = preds.flatten()\n",
    "    \n",
    "    if kind == 'abs':\n",
    "        output=(truth==preds).sum()\n",
    "    elif kind == 'acc':\n",
    "        output=(truth==preds).sum()/truth.size\n",
    "    \n",
    "    return output\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "<p>(e) Implement the function <b><code>knnclassifier</code></b>, which should perform $k$ nearest neighbor classification on a given test data set. The call <pre>preds=knnclassifier(xTr,yTr,xTe,k)</pre>\n",
    "should output the predictions for the data in <code>xTe</code> i.e. <code>preds[i]</code> will contain the prediction for <code>xTe[i,:]</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "        \n",
    "def knnclassifier(xTr,yTr,xTe,k):\n",
    "    \"\"\"\n",
    "    function preds=knnclassifier(xTr,yTr,xTe,k);\n",
    "    \n",
    "    k-nn classifier \n",
    "    \n",
    "    Input:\n",
    "    xTr = nxd input matrix with n row-vectors of dimensionality d\n",
    "    xTe = mxd input matrix with m row-vectors of dimensionality d\n",
    "    k = number of nearest neighbors to be found\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    preds = predicted labels, ie preds(i) is the predicted label of xTe(i,:)\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    \n",
    "    # fix array shapes\n",
    "    n, d = xTr.shape\n",
    "    yTr = (yTr.flatten()).reshape(n, 1)\n",
    "\n",
    "    #shuffle the training examples\n",
    "    training = np.hstack((xTr, yTr))\n",
    "    np.random.shuffle(training)\n",
    "    yTr = training[:, -1]\n",
    "\n",
    "    #flatten again\n",
    "    yTr = yTr.flatten()\n",
    "    m, d = xTe.shape\n",
    "\n",
    "    #allocate prediction array\n",
    "    preds = np.zeros(m)\n",
    "    \n",
    "    #get k nearest neighbors indices and their distances to each test example\n",
    "    indices, dists = findknn((training[:, :-1]).reshape(n, d), xTe, k)\n",
    "    label_matrix = np.take(yTr, indices)\n",
    "\n",
    "    for i in range(m):\n",
    "        #if k=1, then there is no need to worry about ties among nearest neighbors' labels\n",
    "        if k == 1:\n",
    "            preds[i] = label_matrix[0, i]\n",
    "        #if not, then we have to be careful to deal with the ties among labels of neighbors\n",
    "        else:\n",
    "            unique, count = np.unique(label_matrix[:, i], return_counts=True)\n",
    "            unique = unique[count.argsort()]\n",
    "            count.sort()\n",
    "            #if there exists only one label in k labels or every label is different, we pick that label\n",
    "            if count.size == 1:\n",
    "                preds[i] = unique[-1]\n",
    "            elif count[-1]==1:\n",
    "                preds[i]=label_matrix[0,i]\n",
    "            #check if there exists a tie between two most frequent labels among k labels\n",
    "            elif count[-1] == count[-2]:\n",
    "                reduced = 1\n",
    "                #progressively discard the most distant neighbor's label until tie breaks\n",
    "                while reduced < k - 1:\n",
    "                    unique, count = np.unique(label_matrix[:-reduced, i], return_counts=True)\n",
    "                    unique = unique[count.argsort()]\n",
    "                    count.sort()\n",
    "                    if count.size == 1:\n",
    "                        break\n",
    "                    elif count[-1] == count[-2]:\n",
    "                        reduced = reduced + 1\n",
    "                    else:\n",
    "                        break\n",
    "                preds[0] = unique[-1]\n",
    "            else:\n",
    "                preds[0] = unique[-1]\n",
    "    #raise NotImplementedError('Your code goes here!')\n",
    "    #add this later preds=preds.astype(int)\n",
    "    preds=preds.flatten()\n",
    "    return preds\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>You can compute the actual classification error on the test set by calling\n",
    "<pre>\n",
    ">> analyze(\"acc\",yTe,knnclassifier(xTr,yTr,xTe,3))\n",
    "</pre></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(e) This script runs the $k$-nearest neighbor classifier over the faces and digits data set. The faces data set has $40$ classes, the digits data set $10$. What classification accuracy would you expect from a random classifier?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Face Recognition: (1-nn)\")\n",
    "xTr,yTr,xTe,yTe=loaddata(\"faces.mat\") # load the data\n",
    "t0 = time.time()\n",
    "preds = knnclassifier(xTr,yTr,xTe,1)\n",
    "result=analyze(\"acc\",yTe,preds)\n",
    "t1 = time.time()\n",
    "print(\"You obtained %.2f%% classification acccuracy in %.4f seconds\\n\" % (result*100.0,t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Boundary Visualization\n",
    "To help give you a visual understanding of how the KNN boundary is affected by $k$ and the specific dataset, feel free to play around with the visualization below. Click anywhere in the graph to add a negative class point. Hold down 'p' and click anywhere to add a positive class point. To increase $k$ hold down 'h' and click anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_knn_boundary(knnclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(f) (optional) Sometimes a $k$-NN classifier can result in a draw, when the majority vote is not clearly defined. Can you improve your accuracy by falling back onto $k$-NN with lower $k$ in such a case?</p>\n",
    "\n",
    "<p>(g) Edit the function <b><code>competition</code></b>, which reads in a training and testing set and makes predictions. Inside this function you are free to use any combination or variation of the k-nearest neighbor classifier. Can you beat my submission on our secret training and testing set? </p>\n",
    "\n",
    "<h4>Evaluation</h4>\n",
    "<p>For this project, you will be ranked on the following measures:\n",
    "<ul>\n",
    "<li>Percentage of test cases passed</li>\n",
    "<li>Average of:\n",
    "<ul>\n",
    "  <li>Accuracy on the faces test dataset and</li>\n",
    "  <li>Accuracy on the digits test dataset</li>\n",
    "  <li>Accuracy on the <i>secret</i> test dataset</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the competition data. We will use plotdata function to visualize the first 10 training images from the digit data set and visualize them. Click on images to cycle through the competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_knn_images(findknn, imageType='digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def competition(xTr,yTr,xTe):\n",
    "    \"\"\"\n",
    "    function preds=competition(xTr,yTr,xTe);\n",
    "    \n",
    "    A classifier that outputs predictions for the data set xTe based on \n",
    "    what it has learned from xTr,yTr\n",
    "    \n",
    "    Input:\n",
    "    xTr = nxd input matrix with n column-vectors of dimensionality d\n",
    "    xTe = mxd input matrix with n column-vectors of dimensionality d\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    preds = predicted labels, ie preds(i) is the predicted label of xTe(i,:)\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    if n == 1:\n",
    "        preds = knnclassifier(xTr, yTr, xTe, 1)\n",
    "    elif n > 1:\n",
    "        yTr = yTr.flatten()\n",
    "        yTr = yTr.reshape(n, 1)\n",
    "\n",
    "        # shuffle training examples\n",
    "        shuffled = np.hstack((xTr, yTr))\n",
    "        np.random.shuffle(shuffled)\n",
    "        xTr = (shuffled[:, :-1]).reshape(n, d)\n",
    "        yTr = shuffled[:, -1].reshape(n, 1)\n",
    "\n",
    "        #randomly splitting given training data as 80% training examples and 20% CV examples to pick optimal k\n",
    "        #it is extremely unlikely that training data set will only have one training example, but just in case\n",
    "        new_n = int(max(np.floor(n * 0.8), 1))\n",
    "        train_new = xTr[:new_n, :]\n",
    "        labels_new = yTr[:new_n, :]\n",
    "        test_data_new = xTr[new_n:, :]\n",
    "        test_label_new = yTr[new_n:, :]\n",
    "        m, d = xTe.shape\n",
    "        preds_matrix = np.zeros((15, n - new_n + 1))\n",
    "        #for every k from 1-nn to 15-nn find the optimal k-nn that minimizes the relative error\n",
    "        for k in range(15):\n",
    "            preds_matrix[k, 1:] = knnclassifier(train_new, labels_new, test_data_new, k + 1)\n",
    "            preds_matrix[k, 0] = analyze(\"acc\", test_label_new, preds_matrix[k, 1:])\n",
    "        row = np.argmax(preds_matrix[:, 0])\n",
    "        #finally pick the optimal k and use them with test examples\n",
    "        preds = knnclassifier(xTr, yTr, xTe, row + 1)\n",
    "    return preds\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
